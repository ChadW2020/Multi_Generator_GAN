
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import print_function

import os
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.autograd as autograd
from torch import optim
from torch.utils.data import DataLoader

from models import Generator
from classifier import Classifier
from dataset import MNIST_Data

M = 3
method = "cwi"
save_folder = "save_" + str(M) + "G_" + method + "/"
chart_folder = save_folder + "bar_chart/"
metric_folder = save_folder + "metrics/"
sample_folder = save_folder + "samples/"
if not os.path.exists(chart_folder):
    os.makedirs(chart_folder)
if not os.path.exists(metric_folder):
    os.makedirs(metric_folder)
if not os.path.exists(sample_folder):
    os.makedirs(sample_folder)

NUM_EPOCHS = list(range(0, 11)) + list(range(15, 201, 5))

BATCH_SIZE = 128
INPUT_DIM = 1000
LR = 0.0002
G_STEPS = 1
D_STEPS = 20
LAMBDA = 10
K = 1

CUDA = "cuda"
device = torch.device(CUDA if torch.cuda.is_available() else "cpu")

num_synth_plot = 200

def get_real(i, num):
    real_data = np.load("dataset/MNIST_test/data.npy")
    real_label = np.load("dataset/MNIST_test/label.npy")
    output = real_data[(real_label == i), :]
    total_num = output.shape[0]
    if (num < total_num):
        select = np.random.randint(0, high=total_num, size=num, dtype=int)
        output = output[select, :]
    output = np.reshape(output, [output.shape[0], 1, 28, 28])
    output = torch.Tensor(output).to(device)
    return output

def bar_chart_epoch(num_gen_digit, epoch=-1):
    ### draw bar chart of synthetic distribution
    ### num_gen_digit is now a [M, 10] numpy array
    M = num_gen_digit.shape[0]
    species = tuple(range(0, 10))
    weight_counts = {}
    for m in range(M):
        key = "G" + str(m)
        weight_counts[key] = num_gen_digit[m, :]
    width = 0.5
    fig, ax = plt.subplots()
    bottom = np.zeros(10)
    for boolean, weight_count in weight_counts.items():
        p = ax.bar(species, weight_count, width, label=boolean, bottom=bottom)
        bottom += weight_count
    name = "final.png"
    if (epoch == -1):
        ax.set_title("Number of digits produced by " + str(M) + " generators")
    else:
        ax.set_title("Number of digits produced by " + str(M) + " generators at epoch " + str(epoch))
        name = "epoch_" + str(epoch) + ".png"
    ax.legend(loc="upper right")
    fig.savefig(chart_folder + name)
    plt.close("all")

def max_HA(num_gen_digit):
    """
    We should look at the fraction of source for each digit. There is no such "alternative". For each digit, random variable A represents an image containing this digit is produced by which generator.
    H_i(A) = -sum_(m=1, ..., M){p(a_im) * log(p(a_im))},
    where a_im refers to "digit i is generated by generator m". A lower H_i means that images containing digit i is generated by less generators. The performance of the whole model on this task can then be measured by the digit with worst H_i, reflected by the maximum value of all H_i's,
    """
    M = num_gen_digit.shape[0]# [M, 10]
    total_i = []
    ha = []
    for i in range(10):
        ha.append(0)
        total_i.append(np.sum(num_gen_digit[:, i]))
        for m in range(M):
            p_a_m = num_gen_digit[m][i] / total_i[-1]
            term = 0 if (p_a_m == 0) else p_a_m * np.log(p_a_m)
            ha[-1] = ha[-1] - term
    max_ha = max(ha)
    return max_ha

def neg_HB(num_gen_digit):# [M, 10]
    """
    In the mean time, we hope that the whole model will cover all the 10 digits without any bias. This task can be measured by a negative entropy
    -H(B) = sum_(i=0, ..., 9)(p(b_i) * log(p*(b_i))),
    where b_i refers to "digit i is generated by some generator". A lower value of -H(B) means that 10 digits are more evenly distributed in the combined output of all the generators.
    """
    M = num_gen_digit.shape[0]# [M, 10]
    num_dist = np.sum(num_gen_digit, axis=0)
    total = np.sum(num_dist)
    neg_hb = 0
    for i in range(10):
        p_b_i = num_dist[i] / total
        term = 0 if (p_b_i == 0) else p_b_i * np.log(p_b_i)
        neg_hb = neg_hb + term
    return neg_hb

def kl_div_pq(num_gen_digit):# [M, 10]
    """
    KL(P|Q)=sum_{x}{ P(x) * log(P(x) / Q(x)) }
    P is real distribution, Q is model output
    """
    M = num_gen_digit.shape[0]# [M, 10]
    num_dist = np.sum(num_gen_digit, axis=0)
    total = np.sum(num_dist)
    neg_hb = 0
    for i in range(10):
        q_b_i = num_dist[i] / total
        term = 0.1 * np.log(0.1 / q_b_i)
        neg_hb = neg_hb + term
    return neg_hb

def kl_div_qp(num_gen_digit):# [M, 10]
    """
    KL(P|Q)=sum_{x}{ P(x) * log(P(x) / Q(x)) }
    Q is real distribution, P is model output
    """
    M = num_gen_digit.shape[0]# [M, 10]
    num_dist = np.sum(num_gen_digit, axis=0)
    total = np.sum(num_dist)
    neg_hb = 0
    for i in range(10):
        p_b_i = num_dist[i] / total
        term = 0 if (p_b_i == 0) else p_b_i * np.log(p_b_i / 0.1)
        neg_hb = neg_hb + term
    return neg_hb

def get_synthetic(Gs, n_times):
    ### get synthetic samples from all generators
    M = len(Gs)
    x_fakes = [[] for x in range(M)]
    for m in range(M):
        for i in range(n_times):
            z_noise = torch.normal(0, 0.02, size=(BATCH_SIZE, INPUT_DIM)).to(device)
            x_fake = Gs[m](z_noise)
            x_fakes[m].append(x_fake)
    return x_fakes

def get_classified(x_fakes, C):
    ### get predicted label from the classifier
    M = len(x_fakes)
    n_batch = len(x_fakes[0])
    list_cats = [[] for x in range(M)]
    for m in range(M):
        x_fake_m = x_fakes[m]
        for i in range(n_batch):
            x_fake = x_fake_m[i]
            y_fake = C(x_fake)
            y_fake = y_fake.argmax(dim=1, keepdim=False)
            list_cats[m] = list_cats[m] + y_fake.cpu().tolist()
    return list_cats

if __name__ == '__main__':
    Gs = []
    for i in range(M):
        G = Generator().to(device)
        Gs.append(G)
    C = Classifier().to(device)
    C = torch.load("classifier/classifier.pth")
    C.eval()
    
    num_gen_digit = np.ones([M, 10]) * 1000
    best_max_ha = 0
    worst_max_ha = max_HA(num_gen_digit)
    best_neg_hb = neg_HB(num_gen_digit)
    worst_neg_hb = 0
    
    rec_epochs = NUM_EPOCHS
    last_epoch = rec_epochs[-1]
    ha_rec = []
    hb_rec = []
    for epoch in rec_epochs:
        for m in range(M):
            Gs[m] = torch.load(save_folder + "G" + str(m) + "_" + str(epoch) + ".pth")
            Gs[m].eval()
        x_fakes = get_synthetic(Gs, 20)
        list_synth_cat = get_classified(x_fakes, C)# nested list, [3, 12800]
        num_gen_digit = np.zeros([M, 10])
        for m in range(M):
            for i in range(10):
                num_gen_digit[m, i] = list_synth_cat[m].count(i)
        bar_chart_epoch(num_gen_digit, epoch=epoch)
        
        max_ha = max_HA(num_gen_digit)
        neg_hb = kl_div_pq(num_gen_digit)
        ha_rec.append(max_ha)
        hb_rec.append(neg_hb)
    fig, ax = plt.subplots(figsize=(15, 9))
    ax.plot(rec_epochs, ha_rec, color="b", marker=".", markersize=3, linewidth=2)
    #ax.hlines(best_max_ha, 0, last_epoch, color="b", linewidth=2, linestyles="dashed")
    ax.hlines(worst_max_ha, 0, last_epoch, color="b", linewidth=2, linestyles="dashed")
    fig.savefig(metric_folder + "metric_max_ha.png")
    plt.close("all")
    fig, ax = plt.subplots(figsize=(15, 9))
    ax.plot(rec_epochs, hb_rec, color="r", marker=".", markersize=3, linewidth=2)
    #ax.hlines(best_neg_hb, 0, last_epoch, color="r", linewidth=2, linestyles="dashed")
    #ax.hlines(worst_neg_hb, 0, last_epoch, color="r", linewidth=2, linestyles="dashed")
    fig.savefig(metric_folder + "metric_neg_hb.png")
    plt.close("all")
    
    # """
    # Evaluation of synthetic quality and module collapse
    # """
    
    for m in range(M):
        Gs[m] = torch.load(save_folder + "G" + str(m) + "_final.pth")
        Gs[m].eval()
    x_fakes = get_synthetic(Gs, 5)
    list_synth_cat = get_classified(x_fakes, C)
    M = len(x_fakes)
    n_batch = len(x_fakes[0])
    for m in range(M):
        
        cat = list_synth_cat[m]
        x_fake = x_fakes[m][0].cpu().detach().numpy()
        for i in range(1, n_batch):
            next_b = x_fakes[m][i].cpu().detach().numpy()
            x_fake = np.concatenate([x_fake, next_b])
        ind_list = np.asarray(list(range(x_fake.shape[0])))
        np.random.shuffle(ind_list)
        for j in range(num_synth_plot):
            ind = ind_list[j]
            img = x_fake[ind, 0, :, :]
            img[0, 0] = 0
            img[-1, -1] = 1
            plt.matshow(img)
            plt.savefig(sample_folder + "gen_" + str(m) + "_" + str(j) + "_cat_" + str(cat[ind]) + ".png")
            plt.close("all")
    x_fakes = sum(x_fakes, [])
    list_synth_cat = sum(list_synth_cat, [])
    x_fake = x_fakes[0].cpu().detach().numpy()
    for i in range(1, len(x_fakes)):
        next_b = x_fakes[i].cpu().detach().numpy()
        x_fake = np.concatenate([x_fake, next_b])
    
    synth_cat = np.asarray(list_synth_cat)
    for i in range(10):
        digit_fake = x_fake[(synth_cat == i), :, :, :]
        num_real = np.sum(((synth_cat == i) * 1))
        digit_real = get_real(i, num_real).cpu().detach().numpy()
        std_fake = np.squeeze(np.std(digit_fake, axis=0))
        std_real = np.squeeze(np.std(digit_real, axis=0))
        max_value = max(np.amax(std_fake), np.amax(std_real))
        std_fake[0, 0] = 0
        std_fake[-1, -1] = max_value
        std_real[0, 0] = 0
        std_real[-1, -1] = max_value
        fig, (ax1, ax2) = plt.subplots(1, 2)
        ax1.matshow(std_fake)
        ax1.set_xlabel('std_heatmap_fake')
        ax2.matshow(std_real)
        ax2.set_xlabel('std_heatmap_real')
        fig.suptitle('digit ' + str(i))
        fig.savefig(sample_folder + "stdhm_" + str(i) + ".png")
        plt.close("all")
